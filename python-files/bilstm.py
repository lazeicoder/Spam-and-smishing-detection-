# -*- coding: utf-8 -*-
"""BiLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124MmBddFg2KjW2Z0E2Kp77BnCcX3fuF6
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from collections import Counter
import random

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

df = pd.read_csv("Dataset_10191.csv")

texts = df["TEXT"].astype(str).tolist()
labels = df["LABEL"].tolist()

# Encode labels: ham, smishing, spam -> 0,1,2
le = LabelEncoder()
y = le.fit_transform(labels)
print("Label mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

X_train, X_test, y_train, y_test = train_test_split(
    texts, y, test_size=0.2, random_state=42, stratify=y
)

def simple_tokenize(text):
    return text.lower().split()

counter = Counter()
for text in X_train:
    counter.update(simple_tokenize(text))

MAX_VOCAB = 30000
most_common = counter.most_common(MAX_VOCAB - 2)

# Reserve 0 for PAD, 1 for UNK
word2idx = {"<PAD>": 0, "<UNK>": 1}
for i, (word, _) in enumerate(most_common, start=2):
    word2idx[word] = i

vocab_size = len(word2idx)
print("Vocab size:", vocab_size)

MAX_LEN = 100

def encode_text(text):
    tokens = simple_tokenize(text)
    ids = [word2idx.get(tok, word2idx["<UNK>"]) for tok in tokens]
    if len(ids) < MAX_LEN:
        ids = ids + [word2idx["<PAD>"]] * (MAX_LEN - len(ids))
    else:
        ids = ids[:MAX_LEN]
    return ids

X_train_enc = np.array([encode_text(t) for t in X_train])
X_test_enc = np.array([encode_text(t) for t in X_test])

class TextDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = TextDataset(X_train_enc, y_train)
test_dataset = TextDataset(X_test_enc, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_idx):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)
        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim,
            batch_first=True,
            bidirectional=True
        )
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, x):
        x = self.embedding(x)           # (B, T, E)
        _, (h_n, _) = self.lstm(x)       # h_n: (2, B, H)
        h_forward = h_n[0]
        h_backward = h_n[1]
        h = torch.cat((h_forward, h_backward), dim=1)  # (B, 2H)
        h = self.dropout(h)
        out = self.fc(h)
        return out

num_classes = len(le.classes_)

model = BiLSTMClassifier(
    vocab_size=vocab_size,
    embed_dim=128,
    hidden_dim=128,
    num_classes=num_classes,
    pad_idx=word2idx["<PAD>"]
).to(device)

print(model)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

EPOCHS = 8

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0.0

    for batch_x, batch_y in train_loader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f}")

model.eval()
all_preds = []
all_true = []

with torch.no_grad():
    for batch_x, batch_y in test_loader:
        batch_x = batch_x.to(device)
        outputs = model(batch_x)
        preds = torch.argmax(outputs, dim=1).cpu().numpy()

        all_preds.extend(preds)
        all_true.extend(batch_y.numpy())

accuracy = accuracy_score(all_true, all_preds)

print("\nTest Accuracy:", accuracy)
print("\nClassification Report:")
print(classification_report(
    all_true,
    all_preds,
    target_names=le.classes_,
    digits=4
))

print("Confusion Matrix:")
print(confusion_matrix(all_true, all_preds))